{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import pybedtools as pbt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Global variables\n",
    "ROOT = \"/users/kcochran/projects/domain_adaptation/\"\n",
    "\n",
    "# shorthand for each TF name\n",
    "tfs = [\"CTCF\", \"CEBPA\", \"Hnf4a\", \"RXRA\"]\n",
    "\n",
    "# plot-acceptable TF names\n",
    "tfs_latex_names = [\"CTCF\", \"CEBPα\", \"HNF4α\", \"RXRα\"]\n",
    "\n",
    "# shorthand names for species to include in plots\n",
    "all_trainspecies = [\"mm10\", \"hg38\"]\n",
    "# plot-acceptable names for species\n",
    "model_names_dict = {\"mm10\" : \"Mouse\", \"hg38\" : \"Human\"}\n",
    "\n",
    "# If you don't care about testing the model on all examples\n",
    "# and want to speed things up, you can set SKIP to not None;\n",
    "# every SKIP-th ***UNBOUND*** example will be used in model evaluation.\n",
    "# Be careful -- make sure that *everywhere* in the code,\n",
    "# the same SKIP value is being used!\n",
    "# Note that since bound sites are so sparse, SKIP only applies\n",
    "# to UNBOUND sites.\n",
    "SKIP = None\n",
    "MODEL_TYPE = \"best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to load DA models\n",
    "\n",
    "from flipGradientTF import GradientReversal\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    y_pred = tensorflow.boolean_mask(y_pred, tensorflow.not_equal(y_true, -1))\n",
    "    y_true = tensorflow.boolean_mask(y_true, tensorflow.not_equal(y_true, -1))\n",
    "    return keras.losses.binary_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### File and Model Loading\n",
    "\n",
    "\n",
    "def get_test_bed_file(species):\n",
    "    # awk -v OFS=\"\\t\" '{ print $1, $2, $3 }' [get_test_bed_seq_file(species)]\n",
    "    return(ROOT + \"data/\" + species + \"/chr2.bed\")\n",
    "\n",
    "\n",
    "def get_alu_intersect_file_chr2():\n",
    "    # awk '$1 == \"chr2\"' [get_alu_file()] > rmsk_alus_chr2.bed\n",
    "    # bedtools intersect -a [get_test_bed_file(species)] -b rmsk_alus_chr2.bed -u -sorted > chr2_alus_intersect.bed\n",
    "    return(ROOT + \"data/hg38/chr2_alus_intersect.bed\")\n",
    "\n",
    "\n",
    "def get_model_file(tf, train_species, run = 1, model_type = MODEL_TYPE):\n",
    "    try:\n",
    "        run_int = int(run)\n",
    "    except:\n",
    "        print(\"Error: You need to pass in a run number.\")\n",
    "    \n",
    "    model_file_prefix = ROOT + \"/\".join([\"models\", tf, train_species + \"_trained\", \"basic_model/\"])\n",
    "    \n",
    "    if model_type == \"earlystop\":\n",
    "        model_file_suffix = \"_run\" + str(run) + \"_earlystop.model\"\n",
    "    elif model_type == \"best\":\n",
    "        model_file_suffix = \"_run\" + str(run) + \"_best.model\"\n",
    "    else:\n",
    "        model_file_suffix = \"_run\" + str(run) + \"_15E_end.model\"\n",
    "    \n",
    "    files = [f for f in os.listdir(model_file_prefix) if f.endswith(model_file_suffix)]\n",
    "    latest_file = max([model_file_prefix + f for f in files], key=os.path.getctime)\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "def load_keras_model(model_file, DA = False):\n",
    "    if DA:\n",
    "        model = keras.models.load_model(model_file,\n",
    "                        custom_objects = {\"GradientReversal\" : GradientReversal,\n",
    "                                          \"custom_loss\" : custom_loss})\n",
    "    else:\n",
    "        model = keras.models.load_model(model_file)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_models_all_runs(tf, train_species):\n",
    "    files = [get_model_file(tf, train_species, run + 1) for run in range(5)]\n",
    "    return [load_keras_model(f) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "from seqdataloader.batchproducers.coordbased.core import Coordinates\n",
    "from seqdataloader.batchproducers.coordbased.coordstovals.fasta import PyfaidxCoordsToVals\n",
    "\n",
    "ROOT = \"/users/kcochran/projects/domain_adaptation/\"\n",
    "\n",
    "GENOMES = {\"mm10\" : \"/users/kcochran/genomes/mm10_no_alt_analysis_set_ENCODE.fasta\",\n",
    "            \"hg38\" : \"/users/kcochran/genomes/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\"}\n",
    "\n",
    "\n",
    "def get_test_bed_file(species, tf):\n",
    "    return(ROOT + \"data/\" + species + \"/\" + tf + \"/chr2.bed\")\n",
    "\n",
    "\n",
    "class UnboundTestGenerator(Sequence):\n",
    "    def __init__(self, batchsize, val_file, skip = None):\n",
    "        self.valfile = val_file\n",
    "        self.get_steps(skip, batchsize)\n",
    "        if \"hg38\" in val_file:\n",
    "            self.converter = PyfaidxCoordsToVals(GENOMES[\"hg38\"])\n",
    "        else:\n",
    "            assert False  # not expecting to use this\n",
    "            self.converter = PyfaidxCoordsToVals(GENOMES[\"mm10\"])\n",
    "        self.batchsize = batchsize\n",
    "        self.get_unbound_coords(skip)\n",
    "        \n",
    "        \n",
    "    def get_steps(self, skip, batchsize):\n",
    "        with open(self.valfile) as f:\n",
    "            lines_in_file = sum([1 for line in f if line.rstrip().split()[-1] == \"0\"])\n",
    "        if skip is None:\n",
    "            self.steps = lines_in_file // batchsize\n",
    "        else:\n",
    "            self.steps = (lines_in_file // skip) // batchsize\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "\n",
    "    def get_unbound_coords(self, skip):\n",
    "        coords = []\n",
    "        line_count = 0\n",
    "        with open(self.valfile) as f:\n",
    "            for line in f:\n",
    "                line_split = line.rstrip().split()\n",
    "                if line_split[-1] == \"0\":\n",
    "                    if skip is None or line_count % skip == 0:\n",
    "                        coords.append(line_split[:3])\n",
    "                    line_count += 1\n",
    "        self.coords = [Coordinates(c[0], int(c[1]), int(c[2])) for c in coords]\n",
    "\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "        batch = self.coords[batch_index * self.batchsize : (batch_index + 1) * self.batchsize]\n",
    "        return self.converter(batch)\n",
    "\n",
    "        \n",
    "class BoundTestGenerator(Sequence):\n",
    "    def __init__(self, batchsize, val_file):\n",
    "        self.valfile = val_file\n",
    "        self.get_steps(batchsize)\n",
    "        if \"hg38\" in val_file:\n",
    "            self.converter = PyfaidxCoordsToVals(GENOMES[\"hg38\"])\n",
    "        else:\n",
    "            assert False  # not expecting to use this\n",
    "            self.converter = PyfaidxCoordsToVals(GENOMES[\"mm10\"])\n",
    "        self.batchsize = batchsize\n",
    "        self.get_bound_coords()\n",
    "        \n",
    "        \n",
    "    def get_steps(self, batchsize):\n",
    "        with open(self.valfile) as f:\n",
    "            lines_in_file = sum([1 for line in f if line.rstrip().split()[-1] == \"1\"])\n",
    "        self.steps = lines_in_file // batchsize\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "    \n",
    "        \n",
    "    def get_bound_coords(self):\n",
    "        with open(self.valfile) as f:\n",
    "            coords_tmp = [line.split()[:3] for line in f if line.rstrip().split()[-1] == \"1\"]\n",
    "            self.coords = [Coordinates(c[0], int(c[1]), int(c[2])) for c in coords_tmp]\n",
    "            assert len(coords_tmp) > 0\n",
    "\n",
    "\n",
    "    def __getitem__(self, batch_index):\n",
    "        batch = self.coords[batch_index * self.batchsize : (batch_index + 1) * self.batchsize]\n",
    "        assert len(batch) > 0\n",
    "        return self.converter(batch)\n",
    "    \n",
    "\n",
    "def get_preds_batched_fast(model, batch_size, bound, val_file, skip = None):\n",
    "    if bound:\n",
    "        generator = BoundTestGenerator(batch_size, val_file)\n",
    "    else:\n",
    "        generator = UnboundTestGenerator(batch_size, val_file, skip)\n",
    "    return np.squeeze(model.predict_generator(generator,\n",
    "                                              use_multiprocessing = True,\n",
    "                                              workers = 8, verbose = 1))\n",
    "\n",
    "\n",
    "def get_avg_preds_on_seqs(models, bound, val_file, skip = None):\n",
    "    all_preds = [get_preds_batched_fast(model, 1024, bound, val_file, skip = skip) for model in models]\n",
    "    avg_preds = np.mean(np.array(all_preds), axis = 0)\n",
    "    return avg_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Alu functions\n",
    "\n",
    "def get_window_starts(bed_file, skip = None, retain_bound = False):\n",
    "    window_starts = []\n",
    "    line_count = 0\n",
    "    with open(bed_file) as f:\n",
    "        for line in f:\n",
    "            line_split = line.rstrip().split()\n",
    "            if skip is None:\n",
    "                window_starts.append(int(line_split[1]))\n",
    "            else:\n",
    "                if line_split[-1] == \"0\":\n",
    "                    if line_count % skip == 0:\n",
    "                        window_starts.append(int(line_split[1]))\n",
    "                    line_count += 1\n",
    "                elif retain_bound and line_split[-1] == \"1\":\n",
    "                    window_starts.append(int(line_split[1]))\n",
    "    return window_starts\n",
    "\n",
    "\n",
    "def get_bound_window_starts(bed_file):\n",
    "    window_starts = []\n",
    "    with open(bed_file) as f:\n",
    "        for line in f:\n",
    "            line_split = line.rstrip().split()\n",
    "            if line_split[-1] == \"1\":\n",
    "                window_starts.append(int(line_split[1]))\n",
    "    return window_starts\n",
    "\n",
    "\n",
    "def get_unbound_window_starts(bed_file, skip = None):\n",
    "    window_starts = []\n",
    "    line_count = 0\n",
    "    with open(bed_file) as f:\n",
    "        for line in f:\n",
    "            line_split = line.rstrip().split()\n",
    "            if line_split[-1] == \"0\":\n",
    "                if skip is None:\n",
    "                    window_starts.append(int(line_split[1]))\n",
    "                elif line_count % skip == 0:\n",
    "                    window_starts.append(int(line_split[1]))\n",
    "                line_count += 1\n",
    "    return window_starts\n",
    "\n",
    "\n",
    "def matches_across_sorted_lists(list_a, list_b):\n",
    "    matches = []\n",
    "    b_index = 0\n",
    "    for a_item in list_a:\n",
    "        while True:\n",
    "            if b_index >= len(list_b):\n",
    "                matches.append(False)\n",
    "                break\n",
    "            if list_b[b_index] > a_item:\n",
    "                matches.append(False)\n",
    "                break\n",
    "            elif list_b[b_index] == a_item:\n",
    "                matches.append(True)\n",
    "                b_index += 1\n",
    "                break\n",
    "            else:\n",
    "                b_index += 1\n",
    "    return matches\n",
    "\n",
    "\n",
    "def get_alu_intersect(tf, skip = None):\n",
    "    test_bed = get_test_bed_file(\"hg38\", tf)\n",
    "    unbound_windows = get_unbound_window_starts(test_bed, skip)\n",
    "    bound_windows = get_bound_window_starts(test_bed)\n",
    "    alu_intersect_bed = get_alu_intersect_file_chr2()\n",
    "    alu_windows = get_window_starts(alu_intersect_bed, None, retain_bound = True)\n",
    "    unbound_labels = matches_across_sorted_lists(unbound_windows, alu_windows)\n",
    "    bound_labels = matches_across_sorted_lists(bound_windows, alu_windows)\n",
    "    return { \"unbound\" : unbound_labels, \"bound\" : bound_labels }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== CTCF =====\n",
      "\n",
      "Loading models...\n",
      "Predicting on bound sequences with mm10-trained models...\n",
      "24/24 [==============================] - 4s 150ms/step\n",
      "24/24 [==============================] - 2s 101ms/step\n",
      "24/24 [==============================] - 2s 92ms/step\n",
      "24/24 [==============================] - 2s 94ms/step\n",
      "24/24 [==============================] - 2s 91ms/step\n",
      "Predicting on unbound sequences with mm10-trained models...\n",
      "4206/4206 [==============================] - 247s 59ms/step\n",
      "4206/4206 [==============================] - 246s 58ms/step\n",
      "4206/4206 [==============================] - 237s 56ms/step\n",
      "4206/4206 [==============================] - 236s 56ms/step\n",
      "4206/4206 [==============================] - 259s 62ms/step\n",
      "Loading models...\n",
      "Predicting on bound sequences with hg38-trained models...\n",
      "24/24 [==============================] - 2s 93ms/step\n",
      "24/24 [==============================] - 2s 92ms/step\n",
      "24/24 [==============================] - 2s 96ms/step\n",
      "24/24 [==============================] - 2s 92ms/step\n",
      "24/24 [==============================] - 2s 98ms/step\n",
      "Predicting on unbound sequences with hg38-trained models...\n",
      "4206/4206 [==============================] - 241s 57ms/step\n",
      "4206/4206 [==============================] - 255s 61ms/step\n",
      "4206/4206 [==============================] - 255s 61ms/step\n",
      "4206/4206 [==============================] - 246s 58ms/step\n",
      "4206/4206 [==============================] - 239s 57ms/step\n",
      "\n",
      "===== CEBPA =====\n",
      "\n",
      "Loading models...\n",
      "Predicting on bound sequences with mm10-trained models...\n",
      "25/25 [==============================] - 2s 92ms/step\n",
      "25/25 [==============================] - 2s 96ms/step\n",
      "25/25 [==============================] - 3s 103ms/step\n",
      "25/25 [==============================] - 3s 111ms/step\n",
      "25/25 [==============================] - 2s 97ms/step\n",
      "Predicting on unbound sequences with mm10-trained models...\n",
      "4205/4205 [==============================] - 268s 64ms/step\n",
      "4205/4205 [==============================] - 268s 64ms/step\n",
      "4205/4205 [==============================] - 236s 56ms/step\n",
      "4205/4205 [==============================] - 240s 57ms/step\n",
      "4205/4205 [==============================] - 234s 56ms/step\n",
      "Loading models...\n",
      "Predicting on bound sequences with hg38-trained models...\n",
      "25/25 [==============================] - 3s 102ms/step\n",
      "25/25 [==============================] - 2s 90ms/step\n",
      "25/25 [==============================] - 3s 103ms/step\n",
      "25/25 [==============================] - 2s 94ms/step\n",
      "25/25 [==============================] - 3s 106ms/step\n",
      "Predicting on unbound sequences with hg38-trained models...\n",
      "4205/4205 [==============================] - 235s 56ms/step\n",
      "4205/4205 [==============================] - 237s 56ms/step\n",
      "4205/4205 [==============================] - 242s 58ms/step\n",
      "4205/4205 [==============================] - 254s 60ms/step\n",
      "4205/4205 [==============================] - 232s 55ms/step\n",
      "\n",
      "===== Hnf4a =====\n",
      "\n",
      "Loading models...\n",
      "Predicting on bound sequences with mm10-trained models...\n",
      "31/31 [==============================] - 3s 90ms/step\n",
      "31/31 [==============================] - 3s 88ms/step\n",
      "31/31 [==============================] - 3s 89ms/step\n",
      "31/31 [==============================] - 3s 97ms/step\n",
      "31/31 [==============================] - 3s 91ms/step\n",
      "Predicting on unbound sequences with mm10-trained models...\n",
      "4198/4198 [==============================] - 237s 57ms/step\n",
      "4198/4198 [==============================] - 267s 64ms/step\n",
      "4198/4198 [==============================] - 250s 59ms/step\n",
      "4198/4198 [==============================] - 241s 57ms/step\n",
      "4198/4198 [==============================] - 255s 61ms/step\n",
      "Loading models...\n",
      "Predicting on bound sequences with hg38-trained models...\n",
      "31/31 [==============================] - 3s 86ms/step\n",
      "31/31 [==============================] - 3s 89ms/step\n",
      "31/31 [==============================] - 3s 86ms/step\n",
      "31/31 [==============================] - 3s 85ms/step\n",
      "31/31 [==============================] - 3s 84ms/step\n",
      "Predicting on unbound sequences with hg38-trained models...\n",
      "4198/4198 [==============================] - 246s 59ms/step\n",
      "4198/4198 [==============================] - 261s 62ms/step\n",
      "4198/4198 [==============================] - ETA:  - 257s 61ms/step\n",
      "4198/4198 [==============================] - 268s 64ms/step\n",
      "4198/4198 [==============================] - 279s 66ms/step\n",
      "\n",
      "===== RXRA =====\n",
      "\n",
      "Loading models...\n",
      "Predicting on bound sequences with mm10-trained models...\n",
      "67/67 [==============================] - 5s 70ms/step\n",
      "67/67 [==============================] - 5s 70ms/step\n",
      "67/67 [==============================] - 5s 67ms/step\n",
      "67/67 [==============================] - 5s 69ms/step\n",
      "67/67 [==============================] - 5s 71ms/step\n",
      "Predicting on unbound sequences with mm10-trained models...\n",
      "4162/4162 [==============================] - 271s 65ms/step\n",
      "4162/4162 [==============================] - 254s 61ms/step\n",
      "4162/4162 [==============================] - 282s 68ms/step\n",
      "4162/4162 [==============================] - 272s 65ms/step\n",
      "4162/4162 [==============================] - 269s 65ms/step\n",
      "Loading models...\n",
      "Predicting on bound sequences with hg38-trained models...\n",
      "67/67 [==============================] - 5s 69ms/step\n",
      "67/67 [==============================] - 5s 70ms/step\n",
      "67/67 [==============================] - 4s 66ms/step\n",
      "67/67 [==============================] - 5s 75ms/step\n",
      "67/67 [==============================] - 5s 77ms/step\n",
      "Predicting on unbound sequences with hg38-trained models...\n",
      "4162/4162 [==============================] - 274s 66ms/step\n",
      "4162/4162 [==============================] - 286s 69ms/step\n",
      "4162/4162 [==============================] - 280s 67ms/step\n",
      "4162/4162 [==============================] - 277s 67ms/step\n",
      "4162/4162 [==============================] - 262s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "preds_dict = defaultdict(lambda: defaultdict(lambda: dict()))\n",
    "\n",
    "for tf in tfs:\n",
    "    print(\"\\n=====\", tf, \"=====\\n\")\n",
    "    \n",
    "    val_file = get_test_bed_file(\"hg38\", tf)\n",
    "    \n",
    "    for train_species in all_trainspecies:\n",
    "        print(\"Loading models...\")\n",
    "        models = get_models_all_runs(tf, train_species)\n",
    "    \n",
    "        print(\"Predicting on bound sequences with \" + train_species + \"-trained models...\")\n",
    "        preds_dict[\"bound\"][tf][train_species] = get_avg_preds_on_seqs(models, True, val_file)\n",
    "        print(\"Predicting on unbound sequences with \" + train_species + \"-trained models...\")\n",
    "        preds_dict[\"unbound\"][tf][train_species] = get_avg_preds_on_seqs(models, False, val_file,\n",
    "                                                                        skip = SKIP)\n",
    "        del train_species, models\n",
    "        keras.backend.clear_session()\n",
    "del tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4306944 4306944\n",
      "24576 24576\n",
      "4307004 24811\n",
      "(0.120361328125, 0.10725938009787928, 0.0962171052631579, 0.21243879651093675, 0.08514297083736001, 0.6523384948165344)\n",
      "4305920 4305920\n",
      "25600 25600\n",
      "4306211 25604\n",
      "(0.18296875, 0.11457455770850884, 0.0, 0.2120940472651605, 0.2320299366960761, 0.8223509120606746)\n",
      "4298752 4298752\n",
      "31744 31744\n",
      "4299631 32184\n",
      "(0.13580519153225806, 0.09053610503282276, 0.06942148760330578, 0.212506327417818, 0.1601258457195198, 0.8729226361031519)\n",
      "4261888 4261888\n",
      "68608 68608\n",
      "4262836 68979\n",
      "(0.132783348880597, 0.09743946240296605, 0.046528274874731566, 0.2132188832742672, 0.1918434848760481, 0.9691309770100315)\n"
     ]
    }
   ],
   "source": [
    "def calc_row_info(preds_dict, alu_labels, train_species, tf):\n",
    "    species1, species2 = train_species\n",
    "    model1_unbound_preds = preds_dict[\"unbound\"][tf][species1]\n",
    "    model2_unbound_preds = preds_dict[\"unbound\"][tf][species2]\n",
    "    model1_bound_preds = preds_dict[\"bound\"][tf][species1]\n",
    "    model2_bound_preds = preds_dict[\"bound\"][tf][species2]\n",
    "    print(len(model1_unbound_preds), len(model2_unbound_preds))\n",
    "    print(len(model1_bound_preds), len(model2_bound_preds))\n",
    "    print(len(alu_labels[\"unbound\"]), len(alu_labels[\"bound\"]))\n",
    "    alu_labels[\"unbound\"] = alu_labels[\"unbound\"][:len(model1_unbound_preds)]\n",
    "    alu_labels[\"bound\"] = alu_labels[\"bound\"][:len(model1_bound_preds)]\n",
    "    \n",
    "    bound_frac_alu = sum(alu_labels[\"bound\"]) / len(alu_labels[\"bound\"])\n",
    "    unbound_frac_alu = sum(alu_labels[\"unbound\"]) / len(alu_labels[\"unbound\"])\n",
    "    \n",
    "    bound_model1_FNs = 0 \n",
    "    bound_model1_FN_alus = 0\n",
    "    bound_both_models_FNs = 0 \n",
    "    bound_both_models_FN_alus = 0\n",
    "    for i in range(len(model1_bound_preds)):\n",
    "        model1_pred = model1_bound_preds[i]\n",
    "        model2_pred = model2_bound_preds[i]\n",
    "        if model2_pred - model1_pred > 0.5:\n",
    "            bound_model1_FNs += 1\n",
    "            if alu_labels[\"bound\"][i]:\n",
    "                bound_model1_FN_alus += 1\n",
    "        if model2_pred < 0.5 and model1_pred < 0.5:\n",
    "            bound_both_models_FNs += 1\n",
    "            if alu_labels[\"bound\"][i]:\n",
    "                bound_both_models_FN_alus += 1\n",
    "    model1_FN_frac_alu = bound_model1_FN_alus / bound_model1_FNs\n",
    "    both_models_FN_frac_alu = bound_both_models_FN_alus / bound_both_models_FNs\n",
    "    \n",
    "    unbound_model1_FPs = 0 \n",
    "    unbound_model1_FP_alus = 0\n",
    "    unbound_both_models_FPs = 0 \n",
    "    unbound_both_models_FP_alus = 0\n",
    "    for i in range(len(model1_unbound_preds)):\n",
    "        model1_pred = model1_unbound_preds[i]\n",
    "        model2_pred = model2_unbound_preds[i]\n",
    "        if model1_pred - model2_pred > 0.5:\n",
    "            unbound_model1_FPs += 1\n",
    "            if alu_labels[\"unbound\"][i]:\n",
    "                unbound_model1_FP_alus += 1\n",
    "        if model1_pred > 0.5 and model2_pred > 0.5:\n",
    "            unbound_both_models_FPs += 1\n",
    "            if alu_labels[\"unbound\"][i]:\n",
    "                unbound_both_models_FP_alus += 1\n",
    "    model1_FP_frac_alu = unbound_model1_FP_alus / unbound_model1_FPs\n",
    "    both_models_FP_frac_alu = unbound_both_models_FP_alus / unbound_both_models_FPs\n",
    "    \n",
    "    return bound_frac_alu, both_models_FN_frac_alu, model1_FN_frac_alu, unbound_frac_alu, both_models_FP_frac_alu, model1_FP_frac_alu\n",
    "    \n",
    "    \n",
    "def quantify_alu_overlap(preds_dict, train_species = [\"mm10\", \"hg38\"]):\n",
    "    rows = {}\n",
    "    for tf in tfs:\n",
    "        alu_labels = get_alu_intersect(tf)\n",
    "        row = calc_row_info(preds_dict, alu_labels, train_species, tf)\n",
    "        print(row)\n",
    "        rows[tf] = row\n",
    "    return rows\n",
    "\n",
    "\n",
    "alu_overlaps = quantify_alu_overlap(preds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table*}[!pb]\n",
      "\\vspace{-10pt}\n",
      "\\processtable{Caption.\\label{Tab:01}} {\\setlength{\\tabcolsep}{1.2em}\\begin{tabular}{@{}c|cccccc@{}}\\toprule\n",
      "TF & Bound Sites & FNs (Both Models) & FNs (Mouse Only) & Unbound Sites & FPs (Both Models) & FPs (Mouse Only) \\\\\\midrule\n",
      "CTCF & 12.0\\% & 10.7\\% & 9.6\\% & 21.2\\% & 8.5\\% & \\textbf{65.2\\%} \\\\\n",
      "CEBPα & 18.3\\% & 11.5\\% & 0.0\\% & 21.2\\% & 23.2\\% & \\textbf{82.2\\%} \\\\\n",
      "HNF4α & 13.6\\% & 9.1\\% & 6.9\\% & 21.3\\% & 16.0\\% & \\textbf{87.3\\%} \\\\\n",
      "RXRα & 13.3\\% & 9.7\\% & 4.7\\% & 21.3\\% & 19.2\\% & \\textbf{96.9\\%} \\\\\\botrule\n",
      "\\end{tabular}}{}\n",
      "\\vspace*{-10pt}\n",
      "\\end{table*}\n"
     ]
    }
   ],
   "source": [
    "def print_table(rows, header = None, row_order = None):\n",
    "    print(r'\\begin{table*}[!pb]')\n",
    "    print(r'\\vspace{-10pt}')\n",
    "    print(r'\\processtable{Caption.\\label{Tab:01}} {\\setlength{\\tabcolsep}{1.2em}\\begin{tabular}{@{}c|cccccc@{}}\\toprule')\n",
    "    \n",
    "    if header is None:\n",
    "        header = \"TF & Bound Sites & FNs (Both Models) & FNs (Mouse Only) & Unbound Sites & FPs (Both Models) & FPs (Mouse Only)\"\n",
    "    print(header + r' \\\\\\midrule')\n",
    "    \n",
    "    if row_order is None:\n",
    "        row_order = list(rows.keys())\n",
    "    last_row = row_order[-1]\n",
    "    for row_key in row_order:\n",
    "        row_as_str = [\"%0.1f\" % (100 * num) + r'\\%' for num in rows[row_key]]\n",
    "        row_as_str[-1] = r'\\textbf{' + row_as_str[-1] + r'}'\n",
    "        tf_fancy_name = tfs_latex_names[tfs.index(row_key)]\n",
    "        if row_key is not last_row:\n",
    "            print(tf_fancy_name + \" & \" + \" & \".join(row_as_str) + r' \\\\')\n",
    "        else:\n",
    "            print(tf_fancy_name + \" & \" + \" & \".join(row_as_str) + r' \\\\\\botrule')\n",
    "        \n",
    "    #row_as_str = [\"%0.1f\" % (100 * num) + r'\\%' for num in rows[row_order[-1]]]\n",
    "    #tf_fancy_name = tfs_latex_names[tfs.index(row_order[-1])]\n",
    "    \n",
    "    print(r'\\end{tabular}}{}')\n",
    "    print(r'\\vspace*{-10pt}')\n",
    "    print(r'\\end{table*}')\n",
    "    \n",
    "print_table(alu_overlaps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_da2)",
   "language": "python",
   "name": "conda_da2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
