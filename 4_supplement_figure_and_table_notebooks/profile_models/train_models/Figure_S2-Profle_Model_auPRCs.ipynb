{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c68b4015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "095d5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENOMES = { \"mm10\" : \"/users/kcochran/genomes/mm10_no_alt_analysis_set_ENCODE.fasta\",\n",
    "            \"hg38\" : \"/users/kcochran/genomes/GRCh38_no_alt_analysis_set_GCA_000001405.15.fasta\" }\n",
    "\n",
    "ROOT = \"/users/kcochran/projects/domain_adaptation_nosexchr/\"\n",
    "PEAKS_DIR = ROOT + \"data/\"\n",
    "BIGWIGS_DIR = ROOT + \"profile_model_data/\"\n",
    "\n",
    "# shorthand names for all model types to include in plots\n",
    "all_trainspecies = [\"mm10\", \"hg38\"]\n",
    "\n",
    "# plot-acceptable names for model types\n",
    "model_names_dict = {\"mm10\" : \"Mouse-trained\",\n",
    "                    \"hg38\" : \"Human-trained\"}\n",
    "\n",
    "\n",
    "tfs = [\"CTCF\", \"CEBPA\", \"Hnf4a\", \"RXRA\"]\n",
    "# plot-acceptable TF names\n",
    "tfs_latex_names = [\"CTCF\", \"CEBPA\", \"HNF4A\", \"RXRA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65f4c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_JITTER = 200\n",
    "INPUT_SEQ_LEN = 2114\n",
    "OUTPUT_PROF_LEN = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d570fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model_arch import *\n",
    "from attr_prior_utils import *   # Alex's code\n",
    "from data_transforms import *\n",
    "from generators import *\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# This is a modified version of the usual generator used\n",
    "# for binary model training. It is now in Pytorch, and it\n",
    "# processes data on the fly instead of all when the generator\n",
    "# is initialized, since the test set is large.\n",
    "\n",
    "class JITGenerator(Dataset):\n",
    "    letter_dict = {\n",
    "        'a':[1,0,0,0],'c':[0,1,0,0],'g':[0,0,1,0],'t':[0,0,0,1],\n",
    "        'n':[0,0,0,0],'A':[1,0,0,0],'C':[0,1,0,0],'G':[0,0,1,0],\n",
    "        'T':[0,0,0,1],'N':[0,0,0,0]}\n",
    "\n",
    "    def __init__(self, filepaths_dict,\n",
    "                 seq_len,\n",
    "                 profile_len,\n",
    "                 max_jitter,\n",
    "                 batch_size = 1024,  # set to max # GPU can hold\n",
    "                 transform = None,\n",
    "                 return_labels = True, return_controls = True):\n",
    "        \n",
    "        for key in filepaths_dict:\n",
    "            setattr(self, key, filepaths_dict[key])\n",
    "        \n",
    "        self.prof_len = profile_len\n",
    "        self.max_jitter = max_jitter\n",
    "        self.transform = transform\n",
    "        self.return_labels = return_labels\n",
    "        self.return_controls = return_controls\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.set_len()\n",
    "        self.coords = self.get_coords()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "    def set_len(self):\n",
    "        with open(self.peakfile) as f:\n",
    "            self.len = math.ceil(sum([1 for _ in f]) / self.batch_size)\n",
    "\n",
    "\n",
    "    def get_coords(self):\n",
    "        # this function loads in the coordinates for each\n",
    "        # example in the test file\n",
    "        with open(self.peakfile) as posf:\n",
    "            coords_tmp = [line.split()[:3] for line in posf]  # expecting bed file format\n",
    "        \n",
    "        coords = []\n",
    "        for coord in coords_tmp:\n",
    "            chrom, start, end = coord[0], int(coord[1]), int(coord[2])\n",
    "            # since the peak file may not have the right window size,\n",
    "            # this function ensures the window is the correct length\n",
    "            window_start, window_end = expand_window(start, end,\n",
    "                                                     self.seq_len + 2 * self.max_jitter)\n",
    "            coords.append((coord[0], window_start, window_end))  # no strand consideration\n",
    "        return coords\n",
    "            \n",
    "\n",
    "    def get_profiles_and_logcounts(self, coords, pos_bw_file, neg_bw_file):\n",
    "        profiles = []\n",
    "        logcounts = []\n",
    "        for chrom, start, end in coords:\n",
    "            # we need to edit the start and end coords, to get a profile\n",
    "            # that is the right length to match the model's output size\n",
    "            # this is smaller than the input size (the size written in the coords files)\n",
    "            # because of the model's receptive field and deconv layer kernel width\n",
    "            prof_start, prof_end = expand_window(start, end, self.prof_len + 2 * self.max_jitter)\n",
    "\n",
    "            # pyBigWig can read from bigWig files and fetch data at a specific genomic region\n",
    "            # we have two bigWig readers open, one for each DNA strand\n",
    "            with pyBigWig.open(pos_bw_file) as pos_bw_reader:\n",
    "                # read in profile values for the positive strand\n",
    "                pos_profile = np.array(pos_bw_reader.values(chrom, prof_start, prof_end))\n",
    "            with pyBigWig.open(neg_bw_file) as neg_bw_reader:\n",
    "                # read in profile values for the negative strand\n",
    "                neg_profile = np.array(neg_bw_reader.values(chrom, prof_start, prof_end))\n",
    "\n",
    "            # pyBigWig sometimes returns nan when the real data is just 0\n",
    "            pos_profile[np.isnan(pos_profile)] = 0\n",
    "            neg_profile[np.isnan(neg_profile)] = 0\n",
    "\n",
    "            # stick the strands together in an array of shape (2, profile_len)\n",
    "            profile = np.array([pos_profile, neg_profile])\n",
    "            profiles.append(profile)\n",
    "            \n",
    "            # derive values for the counts task by adding up profile\n",
    "            # we take the log bfor technical reasons -- counts are vaguely\n",
    "            # Poisson or negative-binomial distributed, and it is easier\n",
    "            # for the model to model them in log space because of that\n",
    "            pos_logcount = np.log(np.sum(pos_profile) + 1)\n",
    "            neg_logcount = np.log(np.sum(neg_profile) + 1)\n",
    "\n",
    "            # stick the strands together in an array of shape (2,)\n",
    "            logcount = np.array([pos_logcount, neg_logcount])\n",
    "            logcounts.append(logcount)\n",
    "            \n",
    "        return np.array(profiles), np.array(logcounts)\n",
    "                \n",
    "\n",
    "    def convert(self, coords):\n",
    "        # fetch the sequence for a given site/region in the genome, and then one-hot encode\n",
    "        seqs_onehot = []\n",
    "        with Fasta(self.genome_file) as converter:\n",
    "            for chrom, start, stop in coords:\n",
    "                assert chrom in converter\n",
    "                # get sequence\n",
    "                seq = converter[chrom][start:stop].seq\n",
    "                # convert to one-hot\n",
    "                # this array will have shape (4, seq_len)\n",
    "                # this is transposed relative to other code you've written, Kelly\n",
    "                seq = np.array([self.letter_dict.get(x,[0,0,0,0]) for x in seq]).T\n",
    "                seqs_onehot.append(seq)\n",
    "            \n",
    "        return np.array(seqs_onehot)\n",
    "\n",
    "\n",
    "    def __getitem__(self, batch_index):\t\n",
    "        # this function returns one batch's worth of data\n",
    "        coords_batch = self.coords[batch_index * self.batch_size : min((batch_index + 1) * self.batch_size, len(self.coords))]\n",
    "        \n",
    "        # get one-hot sequences for this batch\n",
    "        onehot = self.convert(coords_batch)\n",
    "        assert onehot.shape[0] > 0, onehot.shape\n",
    "        to_return = [onehot]\n",
    "\n",
    "        if self.return_labels:\n",
    "            # get this batch's profiles and logcounts\n",
    "            profiles, logcounts = self.get_profiles_and_logcounts(coords_batch,\n",
    "                                                                  self.pos_bw,\n",
    "                                                                  self.neg_bw)\n",
    "            to_return.extend([profiles, logcounts])\n",
    "            \n",
    "        if self.return_controls:\n",
    "            # get this batch's profiles and logcounts for the control track\n",
    "            control_profiles, control_logcounts = self.get_profiles_and_logcounts(coords_batch,\n",
    "                                                                  self.pos_control_bw,\n",
    "                                                                  self.neg_control_bw)\n",
    "            to_return.extend([control_profiles, control_logcounts])\n",
    "             \n",
    "        # run optional jittering on data, if applicable\n",
    "        if self.transform is not None:\n",
    "            to_return = self.transform(to_return)\n",
    "        \n",
    "        # convert numpy arrays to tensors for Pytorch to use\n",
    "        to_return = [torch.tensor(x.squeeze(), dtype=torch.float) for x in to_return]\n",
    "        return to_return\n",
    "\n",
    "    \n",
    "    \n",
    "# This generator specifically points at the same\n",
    "# test set used for the binary models (chromosome 2).\n",
    "\n",
    "# It extends the generator above in that it also loads\n",
    "# the binary labels for each example, in addition to\n",
    "# the start and stop coordinates for the example.\n",
    "\n",
    "class UnbalancedTestGenerator(JITGenerator):\n",
    "    def __init__(self, species, tf,\n",
    "                 seq_len = INPUT_SEQ_LEN,\n",
    "                 profile_len = OUTPUT_PROF_LEN,\n",
    "                 max_jitter = MAX_JITTER,\n",
    "                 batch_size = 1024,\n",
    "                 transform = None,\n",
    "                 return_labels = True, return_controls = True):\n",
    "        \n",
    "        self.peakfile = PEAKS_DIR + species + \"/\" + tf + \"/chr2.bed\"\n",
    "            \n",
    "        self.pos_bw = BIGWIGS_DIR + species + \"/\" + tf + \"/all_reps.pos.bigWig\"\n",
    "        self.neg_bw = BIGWIGS_DIR + species + \"/\" + tf + \"/all_reps.neg.bigWig\"\n",
    "        self.pos_control_bw = BIGWIGS_DIR + species + \"/\" + tf + \"_control/all_reps.pos.bigWig\"\n",
    "        self.neg_control_bw = BIGWIGS_DIR + species + \"/\" + tf + \"_control/all_reps.neg.bigWig\"\n",
    "        self.prof_len = profile_len\n",
    "        self.max_jitter = max_jitter\n",
    "        self.transform = transform\n",
    "        self.return_labels = return_labels\n",
    "        self.return_controls = return_controls\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.genome_file = GENOMES[species]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.set_len()\n",
    "        self.coords, self.labels = self.get_coords_and_labels()\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "\n",
    "    def get_coords_and_labels(self):\n",
    "        with open(self.peakfile) as posf:\n",
    "            coords_tmp = [line.split()[:4] for line in posf]  # expecting bed file format\n",
    "        \n",
    "        coords = []\n",
    "        labels = []\n",
    "        for coord in coords_tmp:\n",
    "            chrom, start, end, label = coord[0], int(coord[1]), int(coord[2]), int(coord[3])\n",
    "            window_start, window_end = expand_window(start, end,\n",
    "                                                     self.seq_len + 2 * self.max_jitter)\n",
    "            coords.append((chrom, window_start, window_end))  # no strand consideration\n",
    "            labels.append(label)\n",
    "        return coords, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8351b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_logcounts(data_loader, model):\n",
    "    # this function generates a model prediction for each\n",
    "    # example in the data loader.\n",
    "    pred_logcounts = []\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    for seq, control_profile, control_logcounts in data_loader:\n",
    "        seq = seq.squeeze().cuda()\n",
    "        control_profile = pad_control_profile(control_profile.squeeze(), model.untrimmed_prof_len).cuda()\n",
    "        control_logcounts = control_logcounts.squeeze().cuda()\n",
    "        pred_logcount = model((seq, control_profile, control_logcounts))[1].cpu().detach().numpy()\n",
    "        pred_logcounts.append(pred_logcount)\n",
    "    model.cpu()\n",
    "    \n",
    "    return np.array(pred_logcounts).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d0ff0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def format_data_for_seaborn(auPRC_dicts):\n",
    "    # This function re-formats the \"auPRC_dicts\" list of dicts\n",
    "    # into one pandas DataFrame that matches how seaborn expects\n",
    "    # data to be input for the plot we will be making\n",
    "    tf_col = []\n",
    "    species_col = []\n",
    "    auprc_col = []\n",
    "    \n",
    "    for tf in tfs:\n",
    "        tf_col.extend([tf] * len(all_trainspecies))\n",
    "        for species in all_trainspecies:\n",
    "            species_col.append(model_names_dict[species])\n",
    "            auprc_col.append(auPRC_dicts[species][tf])\n",
    "        \n",
    "    return pd.DataFrame({\"TF\":tf_col, \"Species\":species_col, \"auPRC\":auprc_col})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7895ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CTCF mm10 ===\n",
      "Predicting log-counts...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "human_auprc_dict = defaultdict(lambda : dict())\n",
    "for train_species in all_trainspecies:\n",
    "    for tf in tfs:\n",
    "        print(\"=== \" + tf + \" \" + train_species + \" ===\")\n",
    "        model_save_path = ROOT + \"models/profile_models/\" + train_species + \"-trained/\" + tf + \"/bestprof.model\"\n",
    "        model = torch.load(model_save_path)\n",
    "\n",
    "        unbalval_gen = UnbalancedTestGenerator(\"hg38\", tf, \n",
    "                           transform = NoJitter(MAX_JITTER, INPUT_SEQ_LEN, OUTPUT_PROF_LEN),\n",
    "                           return_labels = False, return_controls = True)\n",
    "        unbalval_data_loader = DataLoader(unbalval_gen, batch_size = 1, shuffle = False)\n",
    "        \n",
    "        print(\"Predicting log-counts...\")\n",
    "        pred_logcounts = get_pred_logcounts(unbalval_data_loader, model)\n",
    "        auPRC = average_precision_score(unbalval_gen.labels, np.sum(pred_logcounts, axis = 1))\n",
    "        print(auPRC)\n",
    "        human_auprc_dict[train_species][tf] = auPRC\n",
    "        \n",
    "\n",
    "human_auprc_df = format_data_for_seaborn(human_auprc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15945e78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to avoid re-running, save results to a file\n",
    "human_auprc_df.to_csv(\"hg38_test_profile_model_auPRCs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78952172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb99c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting code\n",
    "\n",
    "# Constants to specify plot appearance details\n",
    "DOT_SIZE = 10\n",
    "FIG_SIZE_UNIT = 5\n",
    "FIG_SIZE = (FIG_SIZE_UNIT + 1.5, FIG_SIZE_UNIT - 1)\n",
    "FIG_SIZE_SMALL = (FIG_SIZE_UNIT, FIG_SIZE_UNIT - 1)\n",
    "COLORS = [\"#0062B8\", \"#FF0145\"]\n",
    "AX_FONTSIZE = 16\n",
    "AXTICK_FONTSIZE = 13\n",
    "TITLESIZE = 15\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "\n",
    "def make_boxplot(df, species, save_files = False,\n",
    "                 fig_size = FIG_SIZE, colors_to_use = COLORS,\n",
    "                 dot_size = DOT_SIZE, titlesize = TITLESIZE,\n",
    "                 ax_fontsize = AX_FONTSIZE,\n",
    "                 axtick_fontsize = AXTICK_FONTSIZE):\n",
    "    \n",
    "    # This function creates one boxplot using seaborn.\n",
    "    # The data plotted must be stored in a pandas DataFrame (input = \"df\"),\n",
    "    # including 3 columns: TF, Species, and auPRC (case-sensitive names).\n",
    "\n",
    "    # Use the argument save_files to toggle between saving plots\n",
    "    # and outputting them within the notebook.\n",
    "    \n",
    "    # If you want to create a plot containing only a subset of the data\n",
    "    # in your input DataFrame, specify which training species / model types\n",
    "    # to include by listing the model types by name in a list and give\n",
    "    # to the argument \"include\" (see cell below for examples). Plotting\n",
    "    # will follow the order of the model types as they are listed in \"include\".\n",
    "    \n",
    "    \n",
    "    # determine y-axis upper limit of plots\n",
    "    # this is done before data is subsetted to keep axis consistent\n",
    "    # regardless of which subset of data is used\n",
    "    yax_max = max(df[\"auPRC\"]) + 0.05\n",
    "    \n",
    "    df_to_use = df\n",
    "    cols_list = colors_to_use\n",
    "    cols = sns.color_palette(colors_to_use)\n",
    "    \n",
    "    sns.set(style = \"white\")\n",
    "\n",
    "    # plot individual dots\n",
    "    ax = sns.barplot(x = \"TF\", y = \"auPRC\", hue = \"Species\",\n",
    "                       data = df,\n",
    "                       dodge = True,\n",
    "                       palette = cols,\n",
    "                       #size = dot_size,\n",
    "                       #edgecolor = \"0.0001\",\n",
    "                       linewidth = 1)\n",
    "    \n",
    "    labels_list = [model_names_dict[species] for species in all_trainspecies]\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=species,\n",
    "                              markeredgecolor='k', markeredgewidth=1,\n",
    "                          markerfacecolor=c, markersize=10) for c, species in zip(cols_list, labels_list)]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc = 'upper right', ncol = 1)\n",
    "\n",
    "    # add legend\n",
    "    #ax.legend(loc = 'upper right', ncol = 1, frameon = False)\n",
    "\n",
    "    # format and label axes\n",
    "    ax.set_xlabel(\"\", fontsize = 0)\n",
    "    ax.set_ylabel(\"Area Under PRC\", fontsize = ax_fontsize)\n",
    "    ax.set_xticklabels(labels = tfs_latex_names, fontsize = ax_fontsize)\n",
    "    ax.tick_params(axis='y', which='major', pad = -2, labelsize = axtick_fontsize)\n",
    "    plt.ylim(0, yax_max) # limit is hard-coded so that it's constant across all plots\n",
    "    plt.yticks([0, 0.2, 0.4, 0.6])\n",
    "    \n",
    "    # use plot-acceptable version of test data species name\n",
    "    # e.g. \"mm10\" --> \"Mouse\"\n",
    "    title = \"Model Performance, \"\n",
    "    title += r\"$\\bf{\" + model_names_dict[species].replace(\"-trained\", \"\") + \"}$\"\n",
    "    title += \" Test Data\"\n",
    "    plt.title(title, fontsize = titlesize)\n",
    "\n",
    "    if save_files:\n",
    "        plt.savefig(ROOT + \"plots/profile_dotplots_\" + species + \"_test.png\",\n",
    "                    bbox_inches='tight', pad_inches = 0.1, dpi = 300)\n",
    "        plt.savefig(ROOT + \"plots/profile_dotplots_\" + species + \"_test.pdf\",\n",
    "                    bbox_inches='tight', pad_inches = 0.1)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe688f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize' : FIG_SIZE})\n",
    "plt.figure()\n",
    "make_boxplot(human_auprc_df, \"hg38\", save_files = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "mouse_auprc_dict = defaultdict(lambda : dict())\n",
    "for train_species in all_trainspecies:\n",
    "    for tf in tfs:\n",
    "        print(\"=== \" + tf + \" \" + train_species + \" ===\")\n",
    "        model_save_path = ROOT + \"models/profile_models/\" + train_species + \"-trained/\" + tf + \"/bestprof.model\"\n",
    "        model = torch.load(model_save_path)\n",
    "\n",
    "        unbalval_gen = UnbalancedTestGenerator(\"mm10\", tf, \n",
    "                           transform = NoJitter(MAX_JITTER, INPUT_SEQ_LEN, OUTPUT_PROF_LEN),\n",
    "                           return_labels = False, return_controls = True)\n",
    "        unbalval_data_loader = DataLoader(unbalval_gen, batch_size = 1, shuffle = False)\n",
    "\n",
    "        pred_logcounts = get_pred_logcounts(unbalval_data_loader, model)\n",
    "        auPRC = average_precision_score(unbalval_gen.labels, np.sum(pred_logcounts, axis = 1))\n",
    "        print(auPRC)\n",
    "        mouse_auprc_dict[train_species][tf] = auPRC\n",
    "        \n",
    "\n",
    "mouse_auprc_df = format_data_for_seaborn(mouse_auprc_dict)\n",
    "\n",
    "# to avoid re-running, save results to a file\n",
    "mouse_auprc_df.to_csv(\"mm10_test_profile_model_auPRCs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize' : FIG_SIZE})\n",
    "plt.figure()\n",
    "make_boxplot(mouse_auprc_df, \"mm10\", save_files = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14926d63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch] *",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
